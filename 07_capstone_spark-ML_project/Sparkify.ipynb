{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Capstone Project: Sparify\n",
    "\n",
    "## Part 1: Project Overview and Problem Statement\n",
    "\n",
    "Many online services make use of subscriptions where users pay monthly for a product that they are using. When the platform is online, it can be profitable to implement a \"freemium\" model where the service is free to the population (usually with ads) and the subscription allows for the removal of ads.\n",
    "\n",
    "In such situations, being able to predict when a user will unsubscribe so that they can be targeted with promotions to try and convince them to stay can reduce loss of potential revenue over time. In this dataset, the actions for a given user have been recorded and will be used towards this end.\n",
    "\n",
    "However, online platforms tend to generate high volumes of data that can be difficult to keep in memory. Big data solutions such as Spark can help to distribute the workload among several computers, allowing machine learning to be done on larger datasets.\n",
    "\n",
    "Thus, we will use Spark in this project to help with our aim of creating a ML model to predict churn on a user level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Analysis\n",
    "\n",
    "Before doing any machine learning, it is important to load the dataset, perform some cleaning, and do some basic investigation.\n",
    "\n",
    "Depending on correlations, we can decide which features we would like to use in our model (feature selection) and we can also decide which features can be combined or infered to create new features that might provide useful information (feature engineering).\n",
    "\n",
    "\n",
    "### Data Importation and Workspace Setup\n",
    "\n",
    "Below, packages required for this problem will are imported as well as other prepwork for the project.\n",
    "\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Cleaning and analysis will be done on this smaller dataset before doing anything with the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Sparkify\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"mini_sparkify_event_data.json\")\n",
    "df.persist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that there are 12 columns in our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Now that the data has been imported, the first and most obvious thing to check for is missing and duplicated values. \n",
    "\n",
    "Since I am relatively comfortable with SQL, I will try and use that a little. In order to do that I need to create a temp view within the spark object that created the dataframe using the `createOrReplaceTempView` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Temp View\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is complete, we can look at the null values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist 58392\n",
      "auth None\n",
      "firstName 8346\n",
      "gender 8346\n",
      "itemInSession None\n",
      "lastName 8346\n",
      "length 58392\n",
      "level None\n",
      "location 8346\n",
      "method None\n",
      "page None\n",
      "registration 8346\n",
      "sessionId None\n",
      "song 58392\n",
      "status None\n",
      "ts None\n",
      "userAgent 8346\n",
      "userId None\n"
     ]
    }
   ],
   "source": [
    "null_dict = {}\n",
    "\n",
    "for i in df.columns:\n",
    "    \n",
    "    nulls = spark.sql(\"SELECT SUM(CASE WHEN {} IS NULL THEN 1 ELSE 0 END) AS null_{} \\\n",
    "                    FROM df \\\n",
    "                    where {} is null\".format(i, i, i)).collect()[0][\"null_{}\".format(i)]\n",
    "    \n",
    "    print(i, nulls)\n",
    "    null_dict[i] = nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 58k rows where the artist and song names are null. This could be because in this instance, the page that was visited was not intended to play a song. \n",
    "\n",
    "We can also see that there are several columns which are never null;\n",
    "\n",
    "* auth\n",
    "* itemInSession\n",
    "* level\n",
    "* method\n",
    "* page\n",
    "* sessionId\n",
    "* status\n",
    "* ts\n",
    "* userId\n",
    "\n",
    "This tells us that we have users who commit actions during a session. So, the unique key should be something like (userId, ts). We can quickly verify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='98', ts=1538838475000, n_rows=3),\n",
       " Row(userId='95', ts=1539665400000, n_rows=3),\n",
       " Row(userId='100022', ts=1541463867000, n_rows=3),\n",
       " Row(userId='', ts=1538488477000, n_rows=2),\n",
       " Row(userId='118', ts=1540521007000, n_rows=2),\n",
       " Row(userId='152', ts=1543206099000, n_rows=2),\n",
       " Row(userId='30', ts=1543436963000, n_rows=2),\n",
       " Row(userId='36', ts=1539070836000, n_rows=2),\n",
       " Row(userId='9', ts=1539714540000, n_rows=2),\n",
       " Row(userId='132', ts=1542791668000, n_rows=2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for unique key\n",
    "spark.sql(\"SELECT userId, ts, count(*) AS n_rows \\\n",
    "        FROM df \\\n",
    "        GROUP BY 1, 2\\\n",
    "        HAVING count(*) > 1 \\\n",
    "        ORDER BY 3 DESC \\\n",
    "        LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=783)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find number of userid-timestamp combinations that have duplicated rows\n",
    "spark.sql(\"WITH T AS (SELECT userId, ts, count(*) AS n_rows \\\n",
    "        FROM df \\\n",
    "        GROUP BY 1, 2\\\n",
    "        HAVING COUNT(*) > 1 \\\n",
    "        ORDER BY 3 DESC \\\n",
    "        ) \\\n",
    "        SELECT COUNT(*) FROM T\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that my assertion was wrong and that we have almost 800 userId-ts combinations that have at least 2 entries. We can view an example as well using the query below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Mase', auth='Logged In', firstName='Sawyer', gender='M', itemInSession=70, lastName='Larson', length=255.16363, level='free', location='Houston-The Woodlands-Sugar Land, TX', method='PUT', page='NextSong', registration=1538069638000, sessionId=493, song=\"Lookin' At Me [feat. Puff Daddy] (Album Version)\", status=200, ts=1538838475000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='98'),\n",
       " Row(artist=None, auth='Logged In', firstName='Sawyer', gender='M', itemInSession=71, lastName='Larson', length=None, level='free', location='Houston-The Woodlands-Sugar Land, TX', method='GET', page='Home', registration=1538069638000, sessionId=493, song=None, status=200, ts=1538838475000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='98'),\n",
       " Row(artist=None, auth='Logged In', firstName='Sawyer', gender='M', itemInSession=72, lastName='Larson', length=None, level='free', location='Houston-The Woodlands-Sugar Land, TX', method='GET', page='Home', registration=1538069638000, sessionId=493, song=None, status=200, ts=1538838475000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='98')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get an example with duplicated rows\n",
    "spark.sql(\"SELECT * \\\n",
    "        FROM df \\\n",
    "        WHERE userId = 98\\\n",
    "        AND ts = 1538838475000\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we can see that the user went to the next song before visiting the home page twice all with the same timestamp. It's a little difficult to know what to do in this situation.\n",
    "\n",
    "If we were to be joining tables then not knowing the primary key would result in the duplication of values. We won't be doing any joins with this data and so that is not an issue.\n",
    "\n",
    "However, it might introduct noise for our ML model if we have duplicate rows. Hence, it would be worth dropping page visits that happen multiple times for the same userId and ts. This is because visiting the same page within the same timestamp, while it may not strictly be duplicated, can probably be viewed as such as it provides no more interesting information about a user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(n_duplicated=0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the unique key\n",
    "spark.sql(\"WITH T AS (SELECT userId, ts, itemInSession, page, count(*) AS n_rows \\\n",
    "        FROM df \\\n",
    "        GROUP BY 1, 2, 3, 4\\\n",
    "        HAVING count(*) > 1 \\\n",
    "        ORDER BY 5 DESC \\\n",
    "        ) \\\n",
    "        SELECT COUNT(*) AS n_duplicated FROM T\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have found the unique key. It is userId, ts, itemInSession, page. This is because the for each user they might visit multiple pages within a certain timestamp. However, for each page that they load the itemInSession is iterated.\n",
    "\n",
    "However, there is once instance where the itemInSession was duplicated, meaning that the unique key is only achieved once the page is also taken into consideration.\n",
    "\n",
    "This is a little unexpected because it should not be technically possible to visit multiple pages within the same timestamp. So what we'll do is we'll rank the rows and keep only the timestamps with the lowest itemInSession count per user. (For the one event that was duplicated, we'll just pick a row at random.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = spark.sql(\"WITH T AS (SELECT *, ROW_NUMBER() OVER (PARTITION BY userId, ts ORDER BY itemInSession) as rank\\\n",
    "                    FROM df ) \\\n",
    "                    SELECT * FROM T \\\n",
    "                    WHERE rank = 1\")\n",
    "\n",
    "# Create Temp View\n",
    "df_unique.createOrReplaceTempView(\"df_unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(n_duplicated=0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify clean successul\n",
    "spark.sql(\"WITH T AS (SELECT userId, ts, itemInSession, count(*) AS n_rows \\\n",
    "        FROM df_unique \\\n",
    "        GROUP BY 1, 2, 3\\\n",
    "        HAVING count(*) > 1 \\\n",
    "        ORDER BY 4 DESC \\\n",
    "        ) \\\n",
    "        SELECT COUNT(*) AS n_duplicated FROM T\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good! We've gotten rid of events that happen on the same timestamp, which means that we've taken care of duplicated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist=None, auth='Logged Out', firstName=None, gender=None, itemInSession=103, lastName=None, length=None, level='paid', location=None, method='GET', page='Home', registration=None, sessionId=141, song=None, status=200, ts=1538391913000, userAgent=None, userId='', rank=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As investigated above, there are times when we get missing values. The only issue is that this can happen because the page is not playing an artist. Thus, we need to find a way to distinguish between instances where a missing value is acceptable and other times when it is not.\n",
    "\n",
    "To remind ourselves of missing values, here are the instances where this was true below...\n",
    "\n",
    "* artist: 58392\n",
    "* firstName: 8346\n",
    "* gender: 8346\n",
    "* lastName: 8346\n",
    "* length: 58392\n",
    "* location: 8346\n",
    "* registration: 8346\n",
    "* song: 58392\n",
    "* userAgent: 8346\n",
    "\n",
    "We can see that the number of null `registration` are 8,346. This means that for the columns `firstName`, `gender`, `lastName`,\n",
    "`location` and `userAgent` Likely do not contain any null values that cannot be explained by missing a registration.\n",
    "\n",
    "The remaining two columns `song` and `artist` will be null then the page does not play music.\n",
    "\n",
    "To assess whether it is necessary to take any action on nulls, the following tests should be carried out:\n",
    "\n",
    "1. All registrations have non-null `firstName`, `gender`, `lastName`, `location` and `userAgent`\n",
    "2. All non-registrations have null columns in these cases\n",
    "3. All non-song-playing pages will have null `song` and `artist` and \n",
    "4. vice versa (as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data cleaning has been completed, the steps will be assembled into a single function that can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Before doing any definitions, some questions about the dataset will be defined and answered. This will give us a better feel for the dataset.\n",
    "\n",
    "Questions will be answered using relevant statistics .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn\n",
    "\n",
    "Now that the preliminary analysis is complete, I will create a column `Churn` to use as the label for your model. \n",
    "\n",
    "\n",
    ">Tip:  I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In this section, the features that were most promising will be assembled into a vector to use for training the model. \n",
    "\n",
    "\n",
    "- TODO: Write a script to extract the necessary features from the smaller subset of data\n",
    "- TODO: Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- TODO: Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
